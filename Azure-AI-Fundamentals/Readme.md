# Common AI Workloads

### Computer Vision

Computer Vision is a key AI workload, primarily concerned with enabling computers to interpret and analyze visual data. This can include tasks like image classification, where the system recognizes and categorizes objects within images. Object detection focuses on identifying and locating multiple objects in visual data, while face recognition enables the identification of individuals based on their facial features. OCR (Optical Character Recognition) allows for extracting text from images, and image analysis provides insights like detecting emotions or understanding the content of scenes.

### Natrual Language Processing

Natural Language Processing (NLP) focuses on enabling machines to understand and process human language. In text classification, the goal is to categorize text into specific categories, which could be used for sentiment analysis, topic modeling, or spam detection. NLP also includes language translation, helping systems automatically translate text from one language to another. Speech recognition converts spoken language into text, and text-to-speech (TTS) synthesizes natural-sounding speech from text input, enabling more interactive and accessible systems.

### Anamoly Detection

Anomaly detection in AI involves identifying patterns that deviate from the expected, often applied to areas like fraud detection, cybersecurity, and health monitoring. This workload relies on identifying outliers in datasets and flagging them for further analysis.

### Predictive Analytics

Predictive Analytics is another important workload that uses historical data to predict future trends. By applying machine learning models, businesses can forecast demand, anticipate equipment failures, and plan more effectively. This workload typically involves time-series forecasting, regression analysis, and pattern recognition.

### Content Moderation

It is used to find adult, hateful, offensive, violent, and other harmful contents and prevent them from being uploaded. We can set the severity level to allow some free speech.

# Guiding Principles Of AI Development

### Fairness

AI Systems should treat everyone equally and avoid affecting similarly situated groups of people in different ways

### Reliability and Safety

It is critical that AI systems operate reliabily, safely, and consistently under normal circumstances and in unexpected conditions

### Privacy and security

AI systems should be secure and respect privacy. The machine learning models on which AI systems are based rely on large volumes of data, which may contain personal details that must be kept private. Even after the models are trained and the system is in production, privacy and security need to be considered. As the system uses new data to make predictions or take action, both the data and decisions made from the data may be subject to privacy or security concerns.

### Inclusiveness

AI systems should empower everyone. AI should bring benefits to all parts of society, regardless of physical ability, gender, sexual orientation, ethnicity, or other factors.

### Transparency

AI systems should be understandable. Users should be made fully aware of the purpose of the system, how it works, and what limitations may be expected.

### Accountability

People should be accountable for AI systems. Designers and developers of AI-based solutions should work within a framework of governance and organizational principles that ensure the solution meets ethical and legal standards that are clearly defined.

# Common Machine Learning Types

### Regression

It is a type of supervised learning method. The ability to predict outcome variables give 1 or more inputs. In this type the result is numeric(price,amount,size).
It is generally finding relations between variables.

### Classification

It is a type of supervised learning method. The goal is to predict a discrete label or category for a given input based on its features. The task is to map input data to one of several predefined classes.

### Clustering

This is a type of unsupervised learning methods. In this we donot have any kinds of labelled data. The model is trained to find underlying relations between the input items and group them into clusters based on these relations.
